Run 2
Dataset tr
Number of steps: 100,000
Multiple contexts: 1
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}
  B
  Environment parameters:
    {'high_reward_prob': 0.8, 'low_reward_prob': 0.2, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.78, 'beta': 2.05, 'tau': 1.43, 'policy': 'probability_matching'}
  C
  Environment parameters:
    {'high_reward_prob': 0.7, 'low_reward_prob': 0.3, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 1.1, 'beta': 1.92, 'tau': 1.54, 'policy': 'probability_matching'}

Run 2
Dataset v
Number of steps: 100,000
Multiple contexts: 1
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}
  B
  Environment parameters:
    {'high_reward_prob': 0.8, 'low_reward_prob': 0.2, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.78, 'beta': 2.05, 'tau': 1.43, 'policy': 'probability_matching'}
  C
  Environment parameters:
    {'high_reward_prob': 0.7, 'low_reward_prob': 0.3, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 1.1, 'beta': 1.92, 'tau': 1.54, 'policy': 'probability_matching'}


Model name: model_seen99M
  Num Parameters: 51136

Tokens seen: 99999744

Total batch size: 6,144

Max steps: 16,276

Dataloader parameters:

File trained on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_2/behavior_run_2tr.txt

File validated on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_2/behavior_run_2v.txt
  Batch size (B): 256
  Sequence length (T): 12
  Steps per epoch: 32
GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 1
  Number of heads: 1
  Embedding size: 64

Run: 2

Filename: '/n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_2/pred_run_2_model_seen99M.txt'
Model used for guessing: model_seen99M

Data guessed on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_2/behavior_run_2v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 1
  Number of heads: 1
  Embedding size: 64
