Run 1
Dataset tr
Number of steps: 100,000
Multiple domains: 1
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}
  B
  Environment parameters:
    {'high_reward_prob': 0.8, 'low_reward_prob': 0.2, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.78, 'beta': 2.05, 'tau': 1.43, 'policy': 'probability_matching'}
  C
  Environment parameters:
    {'high_reward_prob': 0.7, 'low_reward_prob': 0.3, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 1.1, 'beta': 1.92, 'tau': 1.54, 'policy': 'probability_matching'}

Run 1
Dataset v
Number of steps: 100,000
Multiple domains: 1
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}
  B
  Environment parameters:
    {'high_reward_prob': 0.8, 'low_reward_prob': 0.2, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.78, 'beta': 2.05, 'tau': 1.43, 'policy': 'probability_matching'}
  C
  Environment parameters:
    {'high_reward_prob': 0.7, 'low_reward_prob': 0.3, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 1.1, 'beta': 1.92, 'tau': 1.54, 'policy': 'probability_matching'}


Model name: model_seen9M
  Num Parameters: 3163648

Tokens seen: 116,428,800

Total batch size: 6,144

Max steps: 18,950

Dataloader parameters:

File trained on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1tr.txt

File validated on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt
  Batch size (B): 256
  Sequence length (T): 12
  Steps per epoch: 379
GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256

Run: 1

Filename: '/n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_model_seen9M.txt'
Model used for guessing: model_seen9M

Data guessed on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256
Run: 1

Filename: '/n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_model_seen9M_cp0.txt'
Model used for guessing: model_seen9M_cp0

Data guessed on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256
Run: 1

Filename: '/n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_model_seen9M_cp11370.txt'
Model used for guessing: model_seen9M_cp11370

Data guessed on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256
Run: 1

Filename: '/n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_model_seen9M_cp13265.txt'
Model used for guessing: model_seen9M_cp13265

Data guessed on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256
Run: 1

Filename: '/n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_model_seen9M_cp15160.txt'
Model used for guessing: model_seen9M_cp15160

Data guessed on: /n/home10/ccheung/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 4
  Number of heads: 4
  Embedding size: 256
