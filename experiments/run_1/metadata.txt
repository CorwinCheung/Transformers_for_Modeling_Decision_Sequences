Run 1
Dataset tr
Number of steps: 100,000
Multiple contexts: 0
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}

Run 1
Dataset v
Number of steps: 100,000
Multiple contexts: 0
Task parameters:
  A
  Environment parameters:
    {'high_reward_prob': 0.9, 'low_reward_prob': 0.1, 'transition_prob': 0.02}
  Agent parameters:
    {'alpha': 0.56, 'beta': 1.84, 'tau': 1.25, 'policy': 'probability_matching'}


Model name: model_seen999M
  Num Parameters: 51136

Tokens seen: 999997440

Total batch size: 6,144

Max steps: 162,760

Dataloader parameters:

File trained on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1tr.txt

File validated on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt
  Batch size (B): 256
  Sequence length (T): 12
  Steps per epoch: 32
GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 1
  Number of heads: 1
  Embedding size: 64

Run: 1

Filename: '/n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/pred_run_1_model_seen999M.txt'
Model used for guessing: model_seen999M

Data guessed on: /n/home00/cberon/code/Transformers_for_Modeling_Decision_Sequences/experiments/run_1/seqs/behavior_run_1v.txt

GPTConfig parameters:
  Block size: 12
  Vocab size: 4
  Number of layers: 1
  Number of heads: 1
  Embedding size: 64
